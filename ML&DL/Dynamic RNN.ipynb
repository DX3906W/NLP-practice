{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Dynamic RNN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"pWAldK-wh7z0","colab_type":"code","colab":{}},"source":["%tensorflow_version 1.x\n","import tensorflow as tf\n","import random"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pygYDC3-iGz3","colab_type":"code","colab":{}},"source":["class ToySequenceData(object):\n","  \"\"\"\n","  generate sequence of data with dynamic length\n","  this class generate samples for training:\n","  -class 0: linear sequences (i.e. [0, 1, 2, 3, ...])\n","  -class 1: random sequence(i.e. [1,  3, 10, 7, ...])\n","\n","  NOTICE:\n","  we have to pad each sequence to reach 'max_seq_len' for Tensorflow\n","  consistency (we cannot feed a numpy array with inconsistent\n","  dimensions). The dunamic caculation will then be perform thanks to 'seqlen\n","  attribute that records every actual sequencelength.\n","  \"\"\"\n","  def __init__(self, n_samples=1000, max_seq_len=20, min_seq_len=3, max_value=1000):\n","    self.data = []\n","    self.labels = []\n","    self.seqlen = []\n","    for i in range(n_samples):\n","      #Random sequence length\n","      len = random.randint(min_seq_len, max_seq_len)\n","      #Monitor sequence length for Tensorflow dynamic calculation\n","      self.seqlen.append(len)\n","      #Add a random or linear int sequence (50^ prob)\n","      if random.random() < 0.5:\n","        #Generate a linear sequence\n","        rand_start = random.randint(0, max_value - len)\n","        s = [[float(i)/max_value] for i in range(rand_start, rand_start + len)]\n","        #pad sequence for dimension consistency\n","        s += [[0.] for i in range(max_seq_len - len)]\n","        self.data.append(s)\n","        self.labels.append([1., 0.])\n","      else:\n","        #Generate a random sequence\n","        s = [[float(random.randint(0, max_value)) / max_value]\n","             for i in range(len)]\n","        #pad sequence for dimension consistency\n","        s += [[0.] for i in range(max_seq_len - len)]\n","        self.data.append(s)\n","        self.labels.append([0., 1.])\n","    self.batch_id = 0\n","  def next(self, batch_size):\n","    \"\"\"\n","    return a batch of data, when dataset end is reached, start over.\n","    \"\"\"\n","    if self.batch_id == len(self.data):\n","      self.batch_id = 0\n","    batch_data = (self.data[self.batch_id:min(self.batch_id + \n","                      batch_size, len(self.data))])\n","    batch_labels = (self.labels[self.batch_id:min(self.batch_id + \n","                      batch_size, len(self.data))])\n","    batch_seqlen = (self.seqlen[self.batch_id:min(self.batch_id + \n","                      batch_size, len(self.data))])\n","    self.batch_id = min(self.batch_id + batch_size, len(self.data))\n","\n","    return batch_data, batch_labels, batch_seqlen"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qQenIN0HoWFZ","colab_type":"code","colab":{}},"source":["#MODEL\n","#Parameters\n","\n","learning_rate = 0.01\n","training_steps = 10000\n","batch_size = 128\n","display_step = 200\n","\n","#Network parameters\n","seq_max_len = 20  #Sequence max length\n","n_hidden = 64   #hidden layer num of features\n","n_classes = 2   #linear sequence or not\n","\n","trainset = ToySequenceData(n_samples=10000, max_seq_len=seq_max_len)\n","testset = ToySequenceData(n_samples=500, max_seq_len=seq_max_len)\n","\n","#tf Graph input\n","x = tf.placeholder(\"float\", [None, seq_max_len, 1])\n","y = tf.placeholder(\"float\", [None, n_classes])\n","#A placeholder for indicating each sequence length\n","seqlen = tf.placeholder(tf.int32, [None])\n","\n","#Define weights\n","weights = {\n","    'out':tf.Variable(tf.random_normal([n_hidden, n_classes]))\n","}\n","\n","biases = {\n","    'out': tf.Variable(tf.random_normal([n_classes]))\n","}\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oMevDIyUqR-y","colab_type":"code","colab":{}},"source":["def dynamicRNN(x, seqlen, weights, biases):\n","  #prepare data shape to match 'rnn' function requirements\n","  #current data input shape: (batch_size, n_steps, n_input)\n","  #required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n","  x = tf.unstack(x, seq_max_len, 1)\n","\n","  #define a lstm cell with tensorflow\n","  lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden)\n","\n","  #get lstm cell output, proviing 'sequence_length' will perform dynamic\n","  #calculation\n","  outputs, states = tf.contrib.rnn.static_rnn(lstm_cell, x, dtype=tf.float32,\n","        sequence_length=seqlen)\n","  \n","  #When performing dynamic calculation, we must retrieve the last\n","  #dynamically computed output, i.e., if a  sequence length is 10,\n","  #we need to retrive the 10th output.\n","  #However Tensorflow doesn't support advanced indexing yet, so we build\n","  #a custom op that for each sample in batch size, get its length\n","  #and get the corresponding relevant output.\n","\n","  #'output' is a list of output at every timestep, we pack them in\n","  #a Tensorflow and change back dimension to [batch_size, n_step, n_input]\n","  outputs = tf.stack(outputs)\n","  outputs = tf.transpose(outputs, [1, 0, 2])\n","\n","  #Hack to build the indexing and retrive the right output\n","  batch_size = tf.shape(outputs)[0]\n","  #Start indices for each sample\n","  index = tf.range(0, batch_size) * seq_max_len + (seqlen - 1)\n","  #Indexing\n","  outputs = tf.gather(tf.reshape(outputs, [-1, n_hidden]), index)\n","\n","  #Linear activation, using outputs computed above\n","  return tf.matmul(outputs, weights['out']) + biases['out']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1fF1l8ipuOie","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":595},"outputId":"96ad8d5d-63fb-44e3-93f5-b63c6ad21996","executionInfo":{"status":"ok","timestamp":1572375083377,"user_tz":-60,"elapsed":8149,"user":{"displayName":"Jintao Ling","photoUrl":"","userId":"01866243278995557993"}}},"source":["pred = dynamicRNN(x, seqlen=seqlen, weights=weights, biases=biases)\n","\n","#define loss and optimizer\n","loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n","optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n","\n","#Evauate model\n","correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n","accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n","\n","#Initialize the variables \n","init = tf.global_variables_initializer()"],"execution_count":5,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From <ipython-input-4-fe376306fb4a>:8: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From <ipython-input-4-fe376306fb4a>:13: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.add_weight` method instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From <ipython-input-5-c2610afcc9e3>:4: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","\n","Future major versions of TensorFlow will allow gradients to flow\n","into the labels input on backprop by default.\n","\n","See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"adVNdeGZvQU5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":971},"outputId":"346ebb37-052d-4887-e85d-fc23175b0760","executionInfo":{"status":"ok","timestamp":1572375622350,"user_tz":-60,"elapsed":178815,"user":{"displayName":"Jintao Ling","photoUrl":"","userId":"01866243278995557993"}}},"source":["with tf.Session() as sess:\n","  sess.run(init)\n","\n","  for step in range(1, training_steps+1):\n","    batch_x, batch_y, batch_seqlen = trainset.next(batch_size)\n","    #Run optimization op (backprob)\n","    sess.run(optimizer, feed_dict={\n","        x: batch_x, y: batch_y,\n","        seqlen: batch_seqlen})\n","    if step % display_step == 0 or step == 1:\n","      #calculate batch accuracy & loss\n","      acc, cost = sess.run([accuracy, loss], feed_dict={\n","          x: batch_x, y: batch_y,\n","          seqlen: batch_seqlen})\n","      print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n","                  \"{:.6f}\".format(cost) + \", Training Accuracy= \" + \\\n","                  \"{:.5f}\".format(acc))\n","  print(\"Optimization finished!\")\n","\n","  #Calculate accuracy\n","  test_data = testset.data\n","  test_label = testset.labels\n","  test_seqlen = testset.seqlen\n","  print(\"Testing Accuracy: \", \\\n","        sess.run(accuracy, feed_dict={\n","        x: batch_x, y: batch_y,\n","          seqlen: batch_seqlen}))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Step 1, Minibatch Loss= 0.785570, Training Accuracy= 0.53125\n","Step 200, Minibatch Loss= 0.681822, Training Accuracy= 0.57812\n","Step 400, Minibatch Loss= 0.680873, Training Accuracy= 0.60156\n","Step 600, Minibatch Loss= 0.682110, Training Accuracy= 0.59375\n","Step 800, Minibatch Loss= 0.667329, Training Accuracy= 0.60156\n","Step 1000, Minibatch Loss= 0.653641, Training Accuracy= 0.60156\n","Step 1200, Minibatch Loss= 0.610399, Training Accuracy= 0.68750\n","Step 1400, Minibatch Loss= 0.564339, Training Accuracy= 0.72656\n","Step 1600, Minibatch Loss= 0.478908, Training Accuracy= 0.75781\n","Step 1800, Minibatch Loss= 0.486327, Training Accuracy= 0.76562\n","Step 2000, Minibatch Loss= 0.445929, Training Accuracy= 0.78906\n","Step 2200, Minibatch Loss= 0.465927, Training Accuracy= 0.79688\n","Step 2400, Minibatch Loss= 0.473097, Training Accuracy= 0.79688\n","Step 2600, Minibatch Loss= 0.444582, Training Accuracy= 0.79688\n","Step 2800, Minibatch Loss= 0.503272, Training Accuracy= 0.71875\n","Step 3000, Minibatch Loss= 0.455155, Training Accuracy= 0.76562\n","Step 3200, Minibatch Loss= 0.448531, Training Accuracy= 0.76562\n","Step 3400, Minibatch Loss= 0.444236, Training Accuracy= 0.81250\n","Step 3600, Minibatch Loss= 0.452521, Training Accuracy= 0.77344\n","Step 3800, Minibatch Loss= 0.458672, Training Accuracy= 0.78906\n","Step 4000, Minibatch Loss= 0.411421, Training Accuracy= 0.78125\n","Step 4200, Minibatch Loss= 0.398658, Training Accuracy= 0.80469\n","Step 4400, Minibatch Loss= 0.420193, Training Accuracy= 0.79688\n","Step 4600, Minibatch Loss= 0.434768, Training Accuracy= 0.79688\n","Step 4800, Minibatch Loss= 0.393272, Training Accuracy= 0.80469\n","Step 5000, Minibatch Loss= 0.379888, Training Accuracy= 0.85156\n","Step 5200, Minibatch Loss= 0.398960, Training Accuracy= 0.77344\n","Step 5400, Minibatch Loss= 0.438125, Training Accuracy= 0.78125\n","Step 5600, Minibatch Loss= 0.413811, Training Accuracy= 0.78125\n","Step 5800, Minibatch Loss= 0.394266, Training Accuracy= 0.78125\n","Step 6000, Minibatch Loss= 0.361536, Training Accuracy= 0.85938\n","Step 6200, Minibatch Loss= 0.353504, Training Accuracy= 0.82812\n","Step 6400, Minibatch Loss= 0.248360, Training Accuracy= 0.88281\n","Step 6600, Minibatch Loss= 0.432260, Training Accuracy= 0.78906\n","Step 6800, Minibatch Loss= 0.270710, Training Accuracy= 0.92969\n","Step 7000, Minibatch Loss= 0.188054, Training Accuracy= 0.91406\n","Step 7200, Minibatch Loss= 0.197821, Training Accuracy= 0.93750\n","Step 7400, Minibatch Loss= 0.122330, Training Accuracy= 0.99219\n","Step 7600, Minibatch Loss= 0.178322, Training Accuracy= 0.96094\n","Step 7800, Minibatch Loss= 0.109833, Training Accuracy= 0.96875\n","Step 8000, Minibatch Loss= 0.122758, Training Accuracy= 0.96875\n","Step 8200, Minibatch Loss= 0.097757, Training Accuracy= 0.97656\n","Step 8400, Minibatch Loss= 0.124852, Training Accuracy= 0.98438\n","Step 8600, Minibatch Loss= 0.079961, Training Accuracy= 0.98438\n","Step 8800, Minibatch Loss= 0.098894, Training Accuracy= 0.96875\n","Step 9000, Minibatch Loss= 0.084114, Training Accuracy= 0.98438\n","Step 9200, Minibatch Loss= 0.081704, Training Accuracy= 0.96875\n","Step 9400, Minibatch Loss= 0.102077, Training Accuracy= 0.97656\n","Step 9600, Minibatch Loss= 0.096136, Training Accuracy= 0.97656\n","Step 9800, Minibatch Loss= 0.039215, Training Accuracy= 1.00000\n","Step 10000, Minibatch Loss= 0.079299, Training Accuracy= 0.97656\n","Optimization finished!\n","Testing Accuracy:  0.9765625\n"],"name":"stdout"}]}]}