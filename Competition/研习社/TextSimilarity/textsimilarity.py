# -*- coding: utf-8 -*-
"""TextSimilarity

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YMjS3kD92h83SarBO4vHITE7-TYStncc
"""

!pip install transformers

import tensorflow as tf
import pandas as pd
import numpy as np
import seaborn as sns

from sklearn.model_selection import KFold
import gc

train_df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Competition/研习社/TextSimilarity/train.csv')
test_df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Competition/研习社/TextSimilarity/test.csv')

print(train_df.head(5))

train_text_a_len = [len(text) for text in train_df['text_a'].values]
train_text_b_len = [len(text) for text in train_df['text_b'].values]

sns.kdeplot(train_text_a_len)
sns.kdeplot(train_text_b_len)

print(max(train_text_a_len))
print(max(train_text_a_len))

from transformers import RobertaTokenizer, RobertaConfig, TFRobertaPreTrainedModel
from transformers.modeling_tf_roberta import TFRobertaMainLayer
from transformers.modeling_tf_utils import get_initializer

MODEL_NAME = 'roberta-base'
MAX_LEN = 256
tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)

def to_tokens(text_a, text_b, tokenizer):
  output = tokenizer.encode_plus(text_a, text_b, max_length=MAX_LEN, pad_to_max_length=True)
  return output

def select_field(features, field):
  return [feature[field] for feature in features]

tokenizer_train = []
for text_a, text_b in zip(train_df['text_a'].values, train_df['text_b'].values):
  tokenizer_train.append(to_tokens(text_a, text_b, tokenizer))

tokenizer_test = []
for text_a, text_b in zip(test_df['text_a'].values, test_df['text_b'].values):
  tokenizer_test.append(to_tokens(text_a, text_b, tokenizer))

input_ids_train = np.array(select_field(tokenizer_train, 'input_ids'))
attention_masks_train = np.array(select_field(tokenizer_train, 'attention_mask'))

input_ids_test = np.array(select_field(tokenizer_test, 'input_ids'))
attention_masks_test = np.array(select_field(tokenizer_test, 'attention_mask'))

class CustomModel(TFRobertaPreTrainedModel):
  def __init__(self, config, *inputs, **kwargs):
    super(CustomModel, self).__init__(config, *inputs, **kwargs)
    self.roberta = TFRobertaMainLayer(config, name="roberta")
    self.dropout_1 = tf.keras.layers.Dropout(0.3)
    #print(config.num_labels)
    self.classifier = tf.keras.layers.Dense(units=1,
                        name='classifier', 
                        kernel_initializer=get_initializer(
                        config.initializer_range))

  def call(self, inputs, **kwargs):
    outputs = self.roberta(inputs, **kwargs)
    pooled_output = outputs[1]
    pooled_output = self.dropout_1(pooled_output, training=kwargs.get('training', False))
    #print(pooled_output.shape)
    logits = self.classifier(pooled_output)
    outputs = (logits, )  # add hidden states and attention if they are here

    return outputs

def init_model(model_name):

  # strategy = tf.distribute.experimental.TPUStrategy(resolver)
  # with strategy.scope():
  config = RobertaConfig.from_pretrained(model_name, num_labels=1)
  model = CustomModel.from_pretrained(model_name)
  optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)
  # loss = tf.keras.losses.mean_squared_error()
  # metric = tf.keras.metrics.BinaryAccuracy('accuracy')
  model.compile(optimizer=optimizer, loss='mse')

  return model

BATCH_SIZE = 32
EPOCHS = 2
SPLITS = 5
callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', 
                        patience=3, verbose=0, 
                        restore_best_weights=True)]
model_output = np.zeros(len(input_ids_test)).reshape(-1, 1)
# result = []
kfold = KFold(n_splits=5, shuffle=True, random_state=2020)
X, y = input_ids_train, train_df['socre'].values.reshape(-1, 1)
# skf.get_n_splits(X, y)

for i, (train_index, test_index) in enumerate(kfold.split(X, y)):

  X_train, attention_masks_train_stratified = X[train_index], attention_masks_train[train_index]
  X_test, attention_masks_test_stratified =  X[test_index], attention_masks_train[test_index]

  y_train, y_test = y[train_index], y[test_index]
  X_train = X_train[:-divmod(X_train.shape[0], BATCH_SIZE)[1]]
  attention_masks_train_stratified = attention_masks_train_stratified[:-divmod(attention_masks_train_stratified.shape[0], BATCH_SIZE)[1]]
  y_train = y_train[:-divmod(y_train.shape[0], BATCH_SIZE)[1]]

  model = init_model(MODEL_NAME)
  if i == 0:
      print(model.summary())
  model.fit(X_train, y_train,
            validation_data=(X_test, y_test),
            batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=callbacks)

  model_output += model.predict(input_ids_test)[0]
  #print(prediction[:5])
  #result.append(prediction)
  del model
  gc.collect()
  print('='*22 + ' Split ' + str(i+1) + ' finished ' + '='*22)
model_output /= SPLITS

pd.DataFrame({'ID': test_df.index, 'score': model_output}).to_csv('/content/drive/My Drive/Colab Notebooks/Competition/研习社/TextSimilarity/submission.csv', index=False)